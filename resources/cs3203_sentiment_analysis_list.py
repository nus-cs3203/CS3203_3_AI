# -*- coding: utf-8 -*-
"""cs3203_sentiment_analysis_list.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aEIJ1q8X_EjxaKHmaXzcgvTVIxtX-kA4
"""

!pip install wikipedia

import wikipedia

sv = wikipedia.page("Singlish vocabulary")

sv.content

# prompt: find "== List of Singlish words ==" from sv.content and take text till "== See also =="

import re

start_string = "== List of Singlish words =="
end_string = "== See also =="

start_index = sv.content.find(start_string)
end_index = sv.content.find(end_string)


if start_index != -1 and end_index != -1:
    extracted_text = sv.content[start_index:end_index]
    print(extracted_text)
else:
    print("Strings not found in the text.")

type(extracted_text)

# prompt: generate .txt from extracted_text

with open('singlist_list.txt', 'w') as f:
  f.write(extracted_text)

# prompt: read "text" from data file that looks like json looks like: {"smsCorpus": {"@date": "2015.03.09", "@version": 1.2, "message": [{"@id": 10120, "text": {"$": "Bugis oso near wat..."}, "source": {"srcNumber": {"$": 51}, "phoneModel": {"@manufactuer": "unknown", "@smartphone": "unknown"}, "userProfile": {"userID": {"$": 51}, "age": {"$": "unknown"}, "gender": {"$": "unknown"}, "nativeSpeaker": {"$": "unknown"}, "country": {"$": "SG"}, "city": {"$": "unknown"}, "experience": {"$": "unknown"}, "frequency": {"$": "unknown"}, "inputMethod": {"$": "unknown"}}}, "destination": {"@country": "unknown", "destNumber": {"$": "unknown"}}, "messageProfile": {"@language": "en", "@time": "unknown", "@type": "unknown"}, "collectionMethod": {"@collector": "howyijue", "@method": "unknown", "@time": "2003/4"}}, {"@id": 10121, "text": {"$": "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}, "source": {"srcNumber": {"$": 51}, "phoneModel": {"@manufactuer": "unknown", "@smartphone": "unknown"}, "userProfile": {"userID": {"$": 51}, "age": {"$": "unknown"}, "gender": {"$": "unknown"}, "nativeSpeaker": {"$": "unknown"}, "country": {"$": "SG"}, "city": {"$": "unknown"}, "experience": {"$": "unknown"}, "frequency": {"$": "unknown"}, "inputMethod": {"$": "unknown"}}}, "destination": {"@country": "unknown", "destNumber": {"$": "unknown"}}, "messageProfile": {"@language": "en", "@time": "unknown", "@type": "unknown"}, "collectionMethod": {"@collector": "howyijue", "@method": "unknown", "@time": "2003/4"}}, {"@id": 10122, "text": {"$": "I dunno until when... Lets go learn pilates..."}, "source": {"srcNumber": {"$": 51}, "phoneModel": {"@manufactuer": "unknown", "@smartphone": "unknown"}, "userProfile": {"userID": {"$": 51}, "age": {"$": "unknown"}, "gender": {"$": "unknown"},

import json

def extract_text_from_json(file_path):
  """
  Extracts the 'text' values from a JSON file.

  Args:
    file_path: The path to the JSON file.

  Returns:
    A list of strings, where each string is a 'text' value from the JSON data.
    Returns an empty list if the file does not exist or if there's an error
    parsing the JSON data.
  """
  try:
    with open(file_path, 'r') as file:
      data = json.load(file)
      texts = []
      for message in data['smsCorpus']['message']:
        texts.append(message['text']['$'])
      return texts
  except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    return []
  except json.JSONDecodeError:
    print(f"Error: Invalid JSON format in {file_path}")
    return []
  except KeyError:
    print(f"Error: 'text' key not found in JSON data")
    return []

extract_text_from_json("/content/smsCorpus_en_2015.03.09_all.json")

# Extract text from JSON file
extracted_msgs = extract_text_from_json("/content/smsCorpus_en_2015.03.09_all.json")

extracted_msgs

# prompt: convert extracted_msgs to txt WHERE lines arr of format '<STRING>', avoid errors like AttributeError: 'int' object has no attribute 'replace'

# Extract text from JSON file
extracted_msgs = extract_text_from_json("/content/smsCorpus_en_2015.03.09_all.json")

# Write extracted messages to a text file, handling potential errors
try:
    with open('extracted_messages.txt', 'w') as f:
        for msg in extracted_msgs:
            if isinstance(msg, str):  # Check if the message is a string
                f.write(msg.replace('\n', ' ') + '\n') # avoid newlines
            else:
                print(f"Skipping non-string message: {msg}")
                print(msg)
except Exception as e:
    print(f"An error occurred: {e}")

# prompt: print length of extracted_messages.txxt lines

with open('extracted_messages.txt', 'r') as f:
    lines = f.readlines()
    print(len(lines))

# prompt: generate stats on avergae number of words per line

# Calculate average number of words per line
word_counts = []
for line in lines:
    words = line.strip().split()  # Split the line into words
    word_counts.append(len(words))

average_words_per_line = sum(word_counts) / len(lines) if len(lines) > 0 else 0
print(f"Average number of words per line: {average_words_per_line}")

pd.read_csv("/content/df_train__6.csv").head()

# prompt: i will give .txt and modify sentiment scores (currently -1 to 1) to 0 to 2 (+1 each)

import pandas as pd

def modify_sentiment_scores(input_file, output_file):
    """
    Reads a CSV file, modifies sentiment scores, and writes the results to a new file.

    Args:
        input_file: Path to the input CSV file.
        output_file: Path to the output CSV file.
    """
    try:
        df = pd.read_csv(input_file)
        print(df)
        # Check if the 'sentiment' column exists
        if 'sentiment' not in df.columns:
            print("Error: 'sentiment' column not found in the input file.")
            return

        # Modify sentiment scores
        df['sentiment'] = df['sentiment'].apply(lambda x: x + 1)

        # Write the modified DataFrame to a new CSV file
        df.to_csv(output_file, index=False)
        print(f"Modified data saved to {output_file}")

    except FileNotFoundError:
        print(f"Error: Input file '{input_file}' not found.")
    except pd.errors.EmptyDataError:
        print(f"Error: Input file '{input_file}' is empty.")
    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage
modify_sentiment_scores("/content/extracted_messages senti.txt", "/content/extracted_messages senti.txt")

# prompt: .txt comma separated to pandas \ first line is header. custom delimiter

import pandas as pd

def txt_to_pandas(file_path, delimiter=","):
    """
    Reads a .txt file with a header row into a pandas DataFrame.

    Args:
        file_path (str): The path to the .txt file.
        delimiter (str, optional): The delimiter used in the file. Defaults to ",".

    Returns:
        pandas.DataFrame: The DataFrame created from the file. Returns None if an error occurs.
    """
    try:
        df = pd.read_csv(file_path, delimiter=delimiter)  # Using custom delimiter
        return df
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except pd.errors.ParserError:
        print(f"Error: Unable to parse the file. Please check the file format and delimiter.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# Example usage (replace 'your_file.txt' with the actual file path)
# and adjust the delimiter if necessary
file_path = "/content/extracted_messages senti.txt"  # Example file path
df = txt_to_pandas(file_path, delimiter=";")  # Using a comma as the delimiter


if df is not None:
    print(df.head())

df.head()

"""df.to_csv("singlish_messages_sentiments.csv", separator=";")"""

# prompt: subtract 1 from sentiment_score column in df

df['sentiment_score'] = df['sentiment_score'] - 1

df.head()

df.to_csv("singlish_messages_sentiments.csv")

df = pd.read_csv("/content/singlish_messages_sentiments.csv", sep=";")

df.head()

!pip install transformers datasets torch scikit-learn pandas

import pandas as pd

# Load the CSV file
file_path = "/content/singlish_messages_sentiments.csv"  # Update with your actual file path
df = pd.read_csv(file_path, sep=";")

# Check the first few rows
print(df.head())

df = df[["singlish", "sentiment_description"]]

df.head()

# prompt: rename singlish in dataset to text

# Rename the 'singlish' column to 'text'
df = df.rename(columns={'singlish': 'text'})
df = df.rename(columns={'sentiment_description': 'label'})

# Display the updated DataFrame
print(df.head())

# Assuming you want to update the dataset object as well
dataset = Dataset.from_pandas(df)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

from transformers import AutoTokenizer

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Split the dataset
train_testvalid = dataset.train_test_split(test_size=0.2)
test_valid = train_testvalid['test'].train_test_split(test_size=0.5)

train_data = train_testvalid['train']
val_data = test_valid['train']
test_data = test_valid['test']

print(f"Train data size: {len(train_data)}")
print(f"Validation data size: {len(val_data)}")
print(f"Test data size: {len(test_data)}")

import os
os.environ["WANDB_DISABLED"] = "true"

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer", report_to="none")

import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")

label_mapping = {
    "Negative": 0,
    "Neutral": 1,
    "Positive": 2
}

train_data = train_data.map(lambda x: {"label": label_mapping[x["label"]]})
val_data = val_data.map(lambda x: {"label": label_mapping[x["label"]]})

train_data = train_data.map(lambda x: {"label": int(x["label"])})
val_data = val_data.map(lambda x: {"label": int(x["label"])})

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    compute_metrics=compute_metrics,
)

trainer.train()

from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")

# Tokenize the data correctly
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Apply tokenization to train and validation datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# 1. Read CSV file into pandas DataFrame
df = pd.read_csv("/content/singlish_messages_sentiments.csv", sep=";")
df = df[["singlish", "sentiment_description"]]
df = df.rename(columns={'singlish': 'text'})
df = df.rename(columns={'sentiment_description': 'label'})

# 2. Map text column to dataset
dataset = Dataset.from_pandas(df)

# 3. Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")

# 4. Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

dataset = dataset.map(tokenize_function, batched=True)

# 5. Label mapping
label_mapping = {"Negative": 0, "Neutral": 1, "Positive": 2}
dataset = dataset.map(lambda x: {"label": [label_mapping[label] for label in x["label"]]}, batched=True)




# 6. Split dataset into train and validation sets
train_dataset = dataset.train_test_split(test_size=0.2)["train"]
val_dataset = dataset.train_test_split(test_size=0.2)["test"]

# 7. Load model
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest", num_labels=3)

# 8. Define Trainer arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
    report_to="none",
)

# 9. Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=None,  # Optional: Implement compute_metrics function
)

# 10. Train the model
trainer.train()

# prompt: view trainer stats

trainer.evaluate()

# prompt: use finetuned model on sample sentence

from transformers import pipeline

# Load the fine-tuned model
classifier = pipeline("sentiment-analysis", model="/content/results/checkpoint-54")

# Sample sentence
text = "Lovely to meet you"

# Perform sentiment analysis
result = classifier(text)

# Print the result
result

# prompt: save classifier

# Save the trained model
trainer.save_model("./my_singlish_classifier")

# You can then load it using:
# from transformers import pipeline
# classifier = pipeline("sentiment-analysis", model="./my_singlish_classifier")

# prompt: save to drive instead trainer.save_model("./my_singlish_classifier")


# Save the trained model to Google Drive
trainer.save_model("/content/drive/MyDrive/my_singlish_classifier") #  Change 'MyDrive' if your folder is different

# prompt: download /content/my_singlish_classifier locally

from google.colab import files
files.download('/content/my_singlish_classifier')

# prompt: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/my_singlish_classifier'. Use `repo_type` argument if needed. in loading

from transformers import pipeline

# Load the fine-tuned model from Google Drive
classifier = pipeline("sentiment-analysis", model="/content/drive/MyDrive/my_singlish_classifier")

# Sample sentence
text = "Lovely to meet you"

# Perform sentiment analysis
result = classifier(text)

# Print the result
result

result

result[0]['label']

# prompt: read data from /content/filtered_singapore_submissions_with_comments.csv

import pandas as pd

# Load the CSV file into a pandas DataFrame.
df = pd.read_csv("/content/filtered_singapore_submissions_with_comments.csv")

# Now you can work with the DataFrame 'df'.
# For example, to print the first 5 rows:
print(df.head())

# Or to get the number of rows:
print(len(df))

# Or to access a specific column:
print(df.columns)

new_df = df

import pandas as pd

# Assuming 'df' is your DataFrame

# Convert 'created_utc' from Unix timestamp (in seconds) to datetime format
new_df['created_utc'] = pd.to_datetime(new_df['created_utc'], unit='s', utc=True)

new_df.head()

import pandas as pd
import datetime

def process_csv(df, start_date, end_date):
    """
    Filters the DataFrame based on a date range in the 'created_utc' column.

    Args:
        df (pandas.DataFrame): The DataFrame containing the data.
        start_date (datetime.date): The start date as a datetime object.
        end_date (datetime.date): The end date as a datetime object.

    Returns:
        pandas.DataFrame: A DataFrame containing the filtered data,
                          or None if an error occurs.
    """
    try:
        # Ensure the "created_utc" column is in datetime format (in UTC)
        try:
            df['created_utc'] = pd.to_datetime(df['created_utc'], utc=True)  # Important: specify utc=True
        except (KeyError, ValueError) as e:
            print(f"Error converting 'created_utc' column to datetime: {e}")
            return None

        # Convert start_date and end_date to datetime (UTC)
        start_date_dt = pd.to_datetime(start_date, utc=True)
        end_date_dt = pd.to_datetime(end_date, utc=True)

        # Filter the dataframe based on the date range
        df_filtered = df[(df['created_utc'] >= start_date_dt) & (df['created_utc'] <= end_date_dt)]

        return df_filtered

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# Example Usage
start_date = datetime.date(2023, 1, 1)  # Use datetime.date
end_date = datetime.date(2023, 12, 31)  # Use datetime.date

# Assuming `df` is already a pandas DataFrame
filtered_df = process_csv(new_df, start_date, end_date)

filtered_df.head()

# prompt: remove nan and Nan from selftext and title cols by empty string

# Replace NaN and Nan values with empty strings in 'selftext' and 'title' columns
filtered_df['selftext'] = filtered_df['selftext'].fillna('')
filtered_df['title'] = filtered_df['title'].fillna('')

# prompt: clean selftext and title by removing "[removed]"

# Replace "[removed]" with empty strings in 'selftext' and 'title' columns
filtered_df['selftext'] = filtered_df['selftext'].str.replace(r"\[removed\]", "", regex=True)
filtered_df['title'] = filtered_df['title'].str.replace(r"\[removed\]", "", regex=True)

# prompt: combine selftext and title into single col called "combine_title_desc" in filtered_data

# Combine 'selftext' and 'title' into a new column 'combine_title_desc'
filtered_df['combine_title_desc'] = filtered_df['selftext'].astype(str) + ' ' + filtered_df['title'].astype(str)

filtered_df.head()

# prompt: create new cols sentiment_title_selftext_polarity and sentiment_title_selftext_label and sentiment_comments_polarity and sentiment_comments_label

# Assuming 'filtered_df' is your DataFrame

# Create new columns and initialize them with NaN (or any other default value)
filtered_df['sentiment_title_selftext_polarity'] = float('nan')
filtered_df['sentiment_title_selftext_label'] = ""
filtered_df['sentiment_comments_polarity'] = float('nan')
filtered_df['sentiment_comments_label'] = ""

filtered_df.head()

filtered_df['comments'] = filtered_df['comments'].astype(str).str[:150]

filtered_df.head()

filtered_df.shape

filtered_df.to_csv("filtered_data.csv")

import pandas as pd

# Assuming 'classifier' is your sentiment analysis pipeline and 'filtered_df' is your DataFrame

def analyze_sentiment(text):
    """
    Analyzes the sentiment of a given text using the sentiment analysis pipeline.
    """
    try:
        result = classifier(text)[0]
        return result['label'], result['score']
    except Exception as e:
        print(f"Error analyzing sentiment for text '{text}': {e}")
        return "Error", 0  # Return a default value in case of error

def get_sentiment_details(text):
    label, score = analyze_sentiment(text)

    if label == "neutral":
        polarity = 0
    elif label == "positive":
        polarity = score
    elif label == "negative":
        polarity = -score
    else:
        polarity = float('nan')

    return label.title(), polarity

i = 0

# Apply sentiment analysis to both 'combine_title_desc' and 'comments' and update relevant columns
for index, row in filtered_df.iterrows():
    print(i)
    i+=1
    # Analyze 'combine_title_desc'
    title_label, title_score = get_sentiment_details(row['combine_title_desc'])
    filtered_df.loc[index, 'sentiment_title_selftext_label'] = title_label
    filtered_df.loc[index, 'sentiment_title_selftext_polarity'] = title_score

    # Analyze 'comments'
    comment_label, comment_score = get_sentiment_details(row['comments'])
    filtered_df.loc[index, 'sentiment_comments_label'] = comment_label
    filtered_df.loc[index, 'sentiment_comments_polarity'] = comment_score

filtered_df.head()

filtered_df.head()

!pip install transformers datasets

# prompt: read df1 comma seppated but content within "" skip problem lines

import pandas as pd

def read_csv_with_quotes(file_path):
    """Reads a CSV file, handling quoted fields and skipping problematic lines."""
    try:
        df = pd.read_csv(file_path, sep=",", quotechar='"', on_bad_lines='skip', engine='python')
        return df
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except pd.errors.ParserError:
        print(f"Error: Unable to parse the file. Please check the file format and delimiter.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# Example usage:
file_path = "/content/sentiment_scored_2023_data (1) copy.csv"  # Replace with your actual file path
df1 = read_csv_with_quotes(file_path)

if df1 is not None:
    print(df1.head())

df1.head()

df1.shape

# prompt: fix ame 'Dataset' is not defined

from datasets import Dataset

# ... (Your existing code) ...

# Assuming you want to update the dataset object as well
dataset = Dataset.from_pandas(df1)

dataset = Dataset.from_pandas(df1)

# Show the dataset
print(dataset)

def preprocess_data(examples):
    # Concatenate title, selftext, and comments
    inputs = examples["text"]
    targets = examples["poll_question"]
    return {"input_text": inputs, "target_text": targets}

# Apply preprocessing
dataset = dataset.map(preprocess_data, remove_columns=["text", "poll_question"])

# Split dataset into train and test
train_dataset = dataset.train_test_split(test_size=0.1)["train"]
val_dataset = dataset.train_test_split(test_size=0.1)["test"]

from transformers import T5ForConditionalGeneration, T5Tokenizer

model_name = "t5-small"  # or use "t5-base" or "t5-large"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

from transformers import T5ForConditionalGeneration, T5Tokenizer

model_name = "t5-small"  # or use "t5-base" or "t5-large"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    model_inputs = tokenizer(examples["input_text"], truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(examples["target_text"], truncation=True, padding="max_length", max_length=128)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",          # Output directory for model checkpoints
    evaluation_strategy="epoch",     # Evaluate after each epoch
    learning_rate=2e-5,              # Learning rate
    per_device_train_batch_size=8,   # Batch size for training
    per_device_eval_batch_size=8,    # Batch size for evaluation
    num_train_epochs=1,              # Number of training epochs
    weight_decay=0.01,               # Strength of weight decay
    report_to="none",
)

trainer = Trainer(
    model=model,                         # The model to be trained
    args=training_args,                  # Training arguments
    train_dataset=train_dataset,         # Training dataset
    eval_dataset=val_dataset,            # Evaluation dataset
)

trainer.train()

model.save_pretrained("poll-question-generation-model")
tokenizer.save_pretrained("poll-question-generation-model")